{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simhash import Simhash\n",
    "import jieba\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "\n",
    "def load_sougou_content():\n",
    "    # with open(\"news_sohusite_content.txt\") as F:\n",
    "    # 测试阶段仅加载前1w条记录\n",
    "    with open(\"news_sohusite_xml.dat\", encoding=\"gb18030\") as F:\n",
    "        content = F.readlines()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def write_content_full():\n",
    "    content = load_sougou_content()\n",
    "    with open(\"passageList.txt\", mode=\"w\", encoding=\"gb18030\") as f:\n",
    "        count = 0\n",
    "        for i in content:\n",
    "            if \"<content>\" in i:\n",
    "                i = i.replace(\"</content>\", \"\")\n",
    "                i = i.replace(\"<content>\", \"\")\n",
    "                i = i.replace(\"\\n\", \"\")\n",
    "                i = i.replace(\"\\u3000\", \"\")\n",
    "                i = i.replace(\"\\ue40c\", \"\")\n",
    "                if len(i) <= 100:\n",
    "                    continue\n",
    "                f.write(i)\n",
    "                f.write(\"\\n\")\n",
    "                count += 1\n",
    "                print(count)\n",
    "\n",
    "\n",
    "def read_content_full(lines=0):\n",
    "    with open(\"passageList.txt\", mode=\"r\", encoding=\"gb18030\") as f:\n",
    "        if lines:\n",
    "            pl = []\n",
    "            return [f.readline() for i in range(lines)]\n",
    "        else:\n",
    "            lines = f.readlines()\n",
    "            return lines\n",
    "\n",
    "\n",
    "def load_stopwords():\n",
    "    with open(\"stopwords/cn_stopwords.txt\", encoding=\"utf-8\") as F:\n",
    "        stopwords = F.readlines()\n",
    "    return [word.strip() for word in stopwords]\n",
    "\n",
    "\n",
    "def cut_words(passageList):\n",
    "    if isinstance(passageList, str):\n",
    "        return [word for word in jieba.cut(passageList) if word not in stopwords]\n",
    "    else:\n",
    "        return [\n",
    "            [word for word in jieba.cut(passgae) if word not in stopwords]\n",
    "            for passgae in passageList\n",
    "        ]\n",
    "\n",
    "def write_cut_list(passageList):\n",
    "    with open(\"cutPassageList.txt\", mode=\"r+\", encoding=\"gb18030\") as f:\n",
    "        line = len(f.readlines())\n",
    "        count = 0\n",
    "        doTimes = 0\n",
    "        for news in passageList:\n",
    "            count+=1\n",
    "            if count<=line:\n",
    "                continue\n",
    "            seq = \" \".join(cut_words(news))\n",
    "            f.write(seq)\n",
    "            doTimes+=1\n",
    "        print (doTimes)\n",
    "            \n",
    "\n",
    "def read_cut_list(lines=0):\n",
    "    with open(\"cutPassageList.txt\", mode=\"r\", encoding=\"gb18030\") as f:\n",
    "        if lines:\n",
    "            pl = []\n",
    "            return [f.readline() for i in range(lines)]\n",
    "        else:\n",
    "            lines = f.readlines()\n",
    "            return lines\n",
    "\n",
    "def generate_wordcloud(indexList, index0, cutPassageList_space):\n",
    "    os.makedirs(f'results/{index0}')\n",
    "    indexList1 = []\n",
    "    indexList1.append(index0)\n",
    "    indexList1.extend(indexList)\n",
    "    for index in indexList1:\n",
    "        news = cutPassageList_space[index]\n",
    "        wc = WordCloud(\n",
    "            background_color=\"white\",\n",
    "            width=1000,\n",
    "            height=1000,\n",
    "            font_path=\"yhw7.ttf\",\n",
    "        ).generate(news)\n",
    "        wc.to_file(f'results/{index0}/{index}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093388\n"
     ]
    }
   ],
   "source": [
    "stopwords = load_stopwords()\r\n",
    "passageList = read_content_full()\r\n",
    "write_cut_list(passageList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = load_stopwords()\r\n",
    "passageList = read_content_full()\r\n",
    "write_cut_list(passageList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutPassageList_space=[]\n",
    "cutPassageList = read_cut_list()\n",
    "cutPassageList_space = [[word for word in line.split()] for line in cutPassageList]\n",
    "# for i in range(len(cutPassageList)):\n",
    "#     line = cutPassageList[i]\n",
    "#     line = line.replace('\\n','')\n",
    "#     seq=[]\n",
    "#     for word in line.split():\n",
    "#         seq.append(word)\n",
    "#     cutPassageList_space.append(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7d8dd659777c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'index:{index} sim:{sim}\\n{news}\\n\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mgenerate_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpassageIndex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcutPassageList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d85504d9c97a>\u001b[0m in \u001b[0;36mgenerate_wordcloud\u001b[1;34m(indexList, index0, cutPassageList_space)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutPassageList_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'results/{index0}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindex0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mnews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcutPassageList_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         wc = WordCloud(\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import random\n",
    "dataFrameList=[]\n",
    "passageListLen = len(cutPassageList_space)\n",
    "for i in range(200):\n",
    "    passageIndex = random.randint(0,passageListLen-1)\n",
    "    testPassage = cutPassageList_space[passageIndex]\n",
    "    testNewsHash = Simhash(testPassage)\n",
    "    simHashList = []\n",
    "    df = pandas.DataFrame()\n",
    "    for news in cutPassageList_space:\n",
    "        hash = Simhash(news)\n",
    "        score = testNewsHash.distance(hash)\n",
    "        simHashList.append(score)\n",
    "    simHashListSorted = sorted(enumerate(simHashList), key=lambda item: item[1])[:20000]\n",
    "\n",
    "\n",
    "    simList=[]\n",
    "    indexList=[]\n",
    "    for news in simHashListSorted:\n",
    "        index = news[0]\n",
    "        sim = news[1]\n",
    "        indexList.append(index)\n",
    "        simList.append(sim)\n",
    "    df['index'] = indexList\n",
    "    df['sim'] = simList\n",
    "    df.to_csv(f'results/{passageIndex}.csv')\n",
    "\n",
    "\n",
    "    simList=[]\n",
    "    indexList=[]\n",
    "    with open(f'results/{passageIndex}.txt',mode='w+',encoding=\"gb18030\") as f:\n",
    "        f.write(f'{passageIndex}\\n{testPassage}\\n\\n\\n\\n')\n",
    "    \n",
    "    with open(f'results/{passageIndex}.txt',mode='a+',encoding=\"gb18030\") as f:\n",
    "        count = 0\n",
    "        for news in simHashListSorted:\n",
    "            if count >= 50:\n",
    "                break\n",
    "            index = news[0]\n",
    "            sim = news[1]\n",
    "            if sim<=3:\n",
    "                continue\n",
    "            news = cutPassageList[index]\n",
    "            indexList.append(index)\n",
    "            simList.append(sim)\n",
    "            f.write(f'index:{index} sim:{sim}\\n{news}\\n\\n')\n",
    "            count+=1\n",
    "    generate_wordcloud(indexList,passageIndex,cutPassageList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutPassageList = read_cut_list(310000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud([90792,36576,104922],90792,cutPassageList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud([27197,109937,304998],27197,cutPassageList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud([31078,109937,304998],31078,cutPassageList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "metadata": {
   "interpreter": {
    "hash": "8efcb11c19b3219f2cead050734b2ebf587df2a061051559aa52e860fe4590f9"
   }
  },
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}